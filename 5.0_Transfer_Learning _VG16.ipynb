{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5859f3",
   "metadata": {},
   "source": [
    "# Oregon Wildlife - Image Classification Capstone - Model Optimization with Batch Normalization \n",
    "\n",
    "## David Lappin\n",
    "### BrainStation\n",
    "#### 3/1/2023 - 4/10/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd20e1",
   "metadata": {},
   "source": [
    "# Packages Import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004125d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL \n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "import visualkeras\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c19e2",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac1e708",
   "metadata": {},
   "source": [
    "# Import Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f9b8ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set local path to files for basic exploration\n",
    "\n",
    "trainpath = 'data/oregon_wildlife/' # This is the relative path to the data\n",
    "valpath = 'data/oregon_wildlife_validation/' # This is the relative path to the data\n",
    "testpath = 'data/oregon_wildlife_test/' # This is the relative path to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d1b8be",
   "metadata": {},
   "source": [
    "Ensure Data is still in the same state as we left it from the previous EDA before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75347087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588: bald_eagle\n",
      "560: black_bear\n",
      "535: bobcat\n",
      "558: canada_lynx\n",
      "580: columbian_black-tailed_deer\n",
      "518: cougar\n",
      "581: coyote\n",
      "608: deer\n",
      "505: elk\n",
      "513: gray_fox\n",
      "575: gray_wolf\n",
      "420: mountain_beaver\n",
      "536: nutria\n",
      "569: raccoon\n",
      "495: raven\n",
      "602: red_fox\n",
      "426: ringtail\n",
      "540: seals\n",
      "570: sea_lions\n",
      "571: virginia_opossum\n",
      "Total images: 10850\n"
     ]
    }
   ],
   "source": [
    "#check number of images in train folder\n",
    "\n",
    "totalcount_train = []\n",
    "image_files = os.listdir(trainpath)\n",
    "\n",
    "for file in image_files:\n",
    "    \n",
    "    count = len(list(glob(f'data/oregon_wildlife/{file}/*')))\n",
    "    totalcount_train.append(count)\n",
    "    print(f'{count}: {file}')\n",
    "    \n",
    "print(f'Total images: {sum(totalcount_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb0029b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135: bald_eagle\n",
      "135: black_bear\n",
      "135: bobcat\n",
      "135: canada_lynx\n",
      "135: columbian_black-tailed_deer\n",
      "135: cougar\n",
      "135: coyote\n",
      "135: deer\n",
      "135: elk\n",
      "135: gray_fox\n",
      "135: gray_wolf\n",
      "135: mountain_beaver\n",
      "135: nutria\n",
      "135: raccoon\n",
      "135: raven\n",
      "135: red_fox\n",
      "135: ringtail\n",
      "135: seals\n",
      "135: sea_lions\n",
      "135: virginia_opossum\n",
      "Total images: 2700\n"
     ]
    }
   ],
   "source": [
    "#check number of images in Validation folder\n",
    "\n",
    "totalcount_validation = []\n",
    "image_files = os.listdir(valpath)\n",
    "for file in image_files:\n",
    "    \n",
    "    count = len(list(glob(f'data/oregon_wildlife_validation/{file}/*')))\n",
    "    totalcount_validation.append(count)\n",
    "    print(f'{count}: {file}')\n",
    "    \n",
    "print(f'Total images: {sum(totalcount_validation)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dad643b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20: bald_eagle\n",
      "20: black_bear\n",
      "20: bobcat\n",
      "20: canada_lynx\n",
      "20: columbian_black-tailed_deer\n",
      "20: cougar\n",
      "20: coyote\n",
      "20: deer\n",
      "20: elk\n",
      "20: gray_fox\n",
      "20: gray_wolf\n",
      "20: mountain_beaver\n",
      "20: nutria\n",
      "20: raccoon\n",
      "20: raven\n",
      "20: red_fox\n",
      "20: ringtail\n",
      "20: seals\n",
      "20: sea_lions\n",
      "20: virginia_opossum\n",
      "Total images: 400\n"
     ]
    }
   ],
   "source": [
    "#check number of images in test folder\n",
    "\n",
    "totalcount_test = []\n",
    "image_files = os.listdir(testpath)\n",
    "\n",
    "for file in image_files:\n",
    "    \n",
    "    count = len(list(glob(f'data/oregon_wildlife_test/{file}/*')))\n",
    "    totalcount_test.append(count)\n",
    "    print(f'{count}: {file}')\n",
    "    \n",
    "print(f'Total images: {sum(totalcount_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459474d0",
   "metadata": {},
   "source": [
    "### Observations: \n",
    "Everything looks the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15940e7",
   "metadata": {},
   "source": [
    "# Prepare Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21d7890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a67df1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10850 files belonging to 20 classes.\n",
      "Found 2700 files belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  trainpath,\n",
    "  label_mode='int',\n",
    "  seed=7,\n",
    "  image_size=IMG_SIZE,\n",
    "  batch_size=batch_size)\n",
    "\n",
    "\n",
    "val_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  valpath,\n",
    "  label_mode='int',\n",
    "  seed=7,\n",
    "  image_size=IMG_SIZE,\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee987f",
   "metadata": {},
   "source": [
    "# Build Model Components - VG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3eeb491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust the prefetching for performance\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE #Autotune optimizes CPU uptime for each batch\n",
    "\n",
    "train_set = train_set.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE) #cache stores data in memory after 1st epoch for faster iterations\n",
    "val_set = val_set.cache().prefetch(buffer_size=AUTOTUNE) #prefetch allows for overlapping \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4387c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = Sequential([\n",
    "  layers.RandomFlip('horizontal'),\n",
    "  layers.RandomRotation(0.2),\n",
    "  layers.RandomZoom(0.1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8256029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scales\n",
    "\n",
    "preprocess_input = tf.keras.applications.vgg16.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60e1f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "base_model = VGG16(input_shape = (IMG_SHAPE), # Shape of our images\n",
    "    include_top = False, # Leave out the last fully connected layer\n",
    "    weights = 'imagenet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1dee916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ff95849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02ad482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = layers.GlobalAveragePooling2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cce38976",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_layer = layers.Dense(20, activation='softmax', name='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59ded119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "vg_model = tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24e8d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "vg_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f47f28d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " tf.__operators__.getitem_2   (None, 224, 224, 3)      0         \n",
      " (SlicingOpLambda)                                               \n",
      "                                                                 \n",
      " tf.nn.bias_add_2 (TFOpLambd  (None, 224, 224, 3)      0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " prediction (Dense)          (None, 20)                10260     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,724,948\n",
      "Trainable params: 10,260\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "933fdde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set stopping criteria\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.05, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8391f45f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlapp\\anaconda3\\envs\\baseclone\\lib\\site-packages\\keras\\backend.py:5585: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "340/340 [==============================] - 658s 2s/step - loss: 8.0884 - accuracy: 0.0987 - val_loss: 6.1132 - val_accuracy: 0.2048\n",
      "Epoch 2/25\n",
      "340/340 [==============================] - 643s 2s/step - loss: 5.3740 - accuracy: 0.2090 - val_loss: 3.8304 - val_accuracy: 0.3833\n",
      "Epoch 3/25\n",
      "340/340 [==============================] - 641s 2s/step - loss: 3.9946 - accuracy: 0.3157 - val_loss: 2.7430 - val_accuracy: 0.5119\n",
      "Epoch 4/25\n",
      "340/340 [==============================] - 642s 2s/step - loss: 3.2474 - accuracy: 0.3885 - val_loss: 2.1697 - val_accuracy: 0.5907\n",
      "Epoch 5/25\n",
      "  5/340 [..............................] - ETA: 8:47 - loss: 2.6447 - accuracy: 0.4250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23476\\1900044395.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m history = vg_model.fit(   \n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\baseclone\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\baseclone\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\baseclone\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\baseclone\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\baseclone\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\baseclone\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\baseclone\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\baseclone\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\baseclone\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "\n",
    "history = vg_model.fit(   \n",
    "  train_set,            \n",
    "  validation_data=val_set, \n",
    "  epochs=epochs,\n",
    "  callbacks = [callbacks]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f2d62",
   "metadata": {},
   "source": [
    "In our baseline model we have now compiled all the layers. There are ~2million trainable parameters, and we see our output layer is 20 which matches our class labels. \n",
    "\n",
    "Next we can fit our model to the training data using 75 epochs or until early stopped as defined by the callback below. Our model will be validated on the validation split. The test set will be saved for predictions later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f2d523",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "The model stopped (early stopping) at 11/25 epochs. We can now save the model for future loading and begin to visualize and evaluate our baseline model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd5fe4d",
   "metadata": {},
   "source": [
    "Save the model: (note this code is only needed if you plan to save the model weights after running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "95d9828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model weights for use in predictions later if desired\n",
    "\n",
    "vg_model.save('models/vg_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40191bd5",
   "metadata": {},
   "source": [
    "# Preliminary Evaluation\n",
    "\n",
    "We can visualize the model perfmormance.\n",
    "\n",
    "##### SOURCE - plot code base from TF Guide:\n",
    "https://www.tensorflow.org/tutorials/images/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a44272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize model performance after 30 \n",
    "\n",
    "#pull out the accuracy and validation accuracies from model fit history\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "#pull out the loss data from the model fit history\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "#x lab range defined by the # of epochs ran\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "#training vs Validation Accuracies\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "#plt.axvline(31, color = \"black\", linestyle = '--', linewidth = 1, alpha = 0.75)\n",
    "#plt.axvline(41, color = \"black\", linestyle = '--', linewidth = 1, alpha = 0.75)\n",
    "#plt.axhline(0.39, color = \"black\", linestyle = '-', linewidth = 1, alpha = 0.75, label = 'Optimal Epochs (30)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "#visualize loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f2b8c1",
   "metadata": {},
   "source": [
    "#### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab6ad0",
   "metadata": {},
   "source": [
    "# Import Test Data and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636cd0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import test set directory for predictions\n",
    "\n",
    "test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  testpath,\n",
    "  image_size=IMG_SIZE,\n",
    "  label_mode = 'categorical') #since these are not for training we can load as categorical rather than integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c59de7",
   "metadata": {},
   "source": [
    "#### SOURCE\n",
    "\n",
    "Found way to extract multiple predictions across whole dataset from answer by (Frightera, 2021)\n",
    "https://stackoverflow.com/questions/68121629/trying-to-extract-y-val-from-dataset-throws-all-the-input-arrays-must-have-same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3dd984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate predictions on test data\n",
    "\n",
    "predictions = np.array([]) #empty list for predictions\n",
    "labels =  np.array([]) #empty list for labels\n",
    "\n",
    "\n",
    "for x, y in test_set:\n",
    "  \n",
    "  #generate predictions \n",
    "  predictions = np.concatenate([predictions, np.argmax(model.predict(x), axis = -1)]) \n",
    "\n",
    "  #generate true labels\n",
    "  labels = np.concatenate([labels, np.argmax(y.numpy(), axis=-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bec203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a tensorflow confusion matrix from the predictions and true labels above\n",
    "\n",
    "con_mat = tf.math.confusion_matrix(labels=labels, predictions=predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77956647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the values such that they fit on a 0-1 scale\n",
    "normalized_conf_mat = con_mat / con_mat.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ba068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the class names and create temp dataframe to that we can plot using the class names (instead of integers)\n",
    "\n",
    "class_names = test_set.class_names\n",
    "con_mat_df = pd.DataFrame(normalized_conf_mat, index=class_names, columns=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3592f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix using the data frame with class names created above\n",
    "plt.figure(figsize = (9,7))\n",
    "sns.heatmap(con_mat_df,\n",
    "            annot=True,\n",
    "            cbar=False,\n",
    "            cmap=\"rocket_r\",\n",
    "            linewidths=1\n",
    "           )\n",
    "plt.title('Confusion Matrix',size = 25,y=1.01)\n",
    "plt.xlabel(\"Predicted Label\", size = 20)\n",
    "plt.ylabel(\"True Label\", size = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b13b323",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269dfbef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#generate classification report from predictions\n",
    "\n",
    "target_names = test_set.class_names\n",
    "print(classification_report(labels, predictions, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d50aa5c",
   "metadata": {},
   "source": [
    "To sort and look closer at the data we can translate the above information into a pandas dataframe so we can sort/agg/visualize the data easier if desired.\n",
    "\n",
    "##### SOURCE - classification report to pandas df:\n",
    "\n",
    "https://stackoverflow.com/questions/39662398/scikit-learn-output-metrics-classification-report-into-csv-tab-delimited-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#translate the classification report into pd df\n",
    "\n",
    "classification_report_report = classification_report(labels, predictions, target_names = target_names, output_dict = True)\n",
    "classification_report_df = pd.DataFrame(classification_report_report).transpose()\n",
    "classification_report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f44814",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dddfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best f1 scores\n",
    "\n",
    "classification_report_df.sort_values('f1-score', ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a72651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#worst f1 scores\n",
    "\n",
    "classification_report_df.sort_values('f1-score', ascending = False).tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094555df",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a2f9e",
   "metadata": {},
   "source": [
    "# Optional Load for Future Model Predictions:\n",
    "\n",
    "If desired in the future, the code below can be used as an example for loading the model to predict on new data or to continue training. The '.h5' file extension saves the model construction, the trained weights, and time information from `compile()`.\n",
    "\n",
    "##### SOURCE\n",
    "https://www.tensorflow.org/guide/keras/save_and_serialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861daa7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#optional code to re-load and predict on new data\n",
    "\n",
    "load_model = keras.models.load_model(\"models/Reg_model.h5\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e008cbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_path = \"randphotos/foximage1.jpg\"\n",
    "img = image.load_img(img_path)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87604c19",
   "metadata": {},
   "source": [
    "##### SOURCE - For predicting new data below - TF guide\n",
    "https://www.tensorflow.org/tutorials/images/classification#predict_on_new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.keras.utils.load_img(\n",
    "    img_path, target_size=(img_height, img_width)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08fdee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e8ac0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad261e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_path = \"randphotos/eagle3.jpg\"\n",
    "img = image.load_img(img_path)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86060a70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = tf.keras.utils.load_img(\n",
    "    img_path, target_size=(img_height, img_width)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb203c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a3b580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab73327f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_path = \"randphotos/blackbear1.jpg\"\n",
    "img = image.load_img(img_path)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8c9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.keras.utils.load_img(\n",
    "    img_path, target_size=(img_height, img_width)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = load_model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640df5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67e7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ed91a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_path = \"randphotos/daldeer1.jpg\"\n",
    "img = image.load_img(img_path)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.keras.utils.load_img(\n",
    "    img_path, target_size=(img_height, img_width)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb043cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb71bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44732ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb93d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_path = \"randphotos/alfox1.JPG\"\n",
    "img = image.load_img(img_path)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db4d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.keras.utils.load_img(\n",
    "    img_path, target_size=(img_height, img_width)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c7a414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f763bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a2121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa28037d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "511a77fb",
   "metadata": {},
   "source": [
    "# Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f0285a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b14d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf6b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb972a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee64fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseclone",
   "language": "python",
   "name": "baseclone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
